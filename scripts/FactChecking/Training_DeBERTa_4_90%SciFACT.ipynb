{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ea224d",
   "metadata": {},
   "source": [
    "## Training DeBERTa 4 over 90% of transformed SciFact dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f4d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback, IntervalStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc8d7217",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./SciFact_train.csv', 'rb') as fh:\n",
    "    df_train = pd.read_csv(fh)\n",
    "df_train.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f989fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./SciFact_valid.csv', 'rb') as fh:\n",
    "    df_valid = pd.read_csv(fh)\n",
    "df_valid.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8684da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./SciFact_test.csv', 'rb') as fh:\n",
    "    df_test = pd.read_csv(fh)\n",
    "df_test.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08ce58ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>abstract</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Angiotensin converting enzyme inhibitors are a...</td>\n",
       "      <td>Renal considerations in angiotensin converting...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Reducing H3k4me3 methylation induces mouse epi...</td>\n",
       "      <td>MLL1 Inhibition Reprograms Epiblast Stem Cells...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Expression of oncolytic virus antigens as pept...</td>\n",
       "      <td>Detecting and targeting tumor relapse by its r...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Varenicline monotherapy is more effective afte...</td>\n",
       "      <td>Combination varenicline and bupropion SR for t...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acute ablation of Snail in the embryonic corte...</td>\n",
       "      <td>Control of Apoptosis by Asymmetric Cell Divisi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>Charcoal shows no benefit for acute paraquat p...</td>\n",
       "      <td>Effect of activated charcoal hemoperfusion on ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>The risk of breast cancer among parous women d...</td>\n",
       "      <td>Pregnancy characteristics and maternal risk of...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>Inositol lipid 3-phosphatase PTEN converts Ptd...</td>\n",
       "      <td>PTEN Regulates PI(3,4)P2 Signaling Downstream ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>Combination nicotine replacement therapies wit...</td>\n",
       "      <td>Combination varenicline and bupropion SR for t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>Women with a lower birth weight are more likel...</td>\n",
       "      <td>Intrauterine factors and risk of breast cancer...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1093 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  claim  \\\n",
       "0     Angiotensin converting enzyme inhibitors are a...   \n",
       "1     Reducing H3k4me3 methylation induces mouse epi...   \n",
       "2     Expression of oncolytic virus antigens as pept...   \n",
       "3     Varenicline monotherapy is more effective afte...   \n",
       "4     Acute ablation of Snail in the embryonic corte...   \n",
       "...                                                 ...   \n",
       "1088  Charcoal shows no benefit for acute paraquat p...   \n",
       "1089  The risk of breast cancer among parous women d...   \n",
       "1090  Inositol lipid 3-phosphatase PTEN converts Ptd...   \n",
       "1091  Combination nicotine replacement therapies wit...   \n",
       "1092  Women with a lower birth weight are more likel...   \n",
       "\n",
       "                                               abstract  label  \n",
       "0     Renal considerations in angiotensin converting...      1  \n",
       "1     MLL1 Inhibition Reprograms Epiblast Stem Cells...      1  \n",
       "2     Detecting and targeting tumor relapse by its r...      0  \n",
       "3     Combination varenicline and bupropion SR for t...      2  \n",
       "4     Control of Apoptosis by Asymmetric Cell Divisi...      0  \n",
       "...                                                 ...    ...  \n",
       "1088  Effect of activated charcoal hemoperfusion on ...      1  \n",
       "1089  Pregnancy characteristics and maternal risk of...      2  \n",
       "1090  PTEN Regulates PI(3,4)P2 Signaling Downstream ...      1  \n",
       "1091  Combination varenicline and bupropion SR for t...      1  \n",
       "1092  Intrauterine factors and risk of breast cancer...      2  \n",
       "\n",
       "[1093 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_90 = pd.concat([df_train, df_test], ignore_index=True)\n",
    "df_train_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a5b9a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid['text'] = tokenizer.cls_token + df_valid['claim'] + tokenizer.sep_token + df_valid['abstract'] + tokenizer.sep_token\n",
    "df_valid.drop(columns=['claim', 'abstract'], inplace=True)\n",
    "\n",
    "df_train_90['text'] = tokenizer.cls_token + df_train_90['claim'] + tokenizer.sep_token + df_train_90['abstract'] + tokenizer.sep_token\n",
    "df_train_90.drop(columns=['claim', 'abstract'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "060de9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "konacno_train = Dataset.from_pandas(df_train_90)\n",
    "konacno_valid = Dataset.from_pandas(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74c365fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict_90 = datasets.DatasetDict({\n",
    "    'train': konacno_train,\n",
    "    'valid': konacno_valid\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e7cab8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366279d96cc2473a88a25ae3f5f96bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60ddb2f509e47dab464edff59cb396c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_90 = dataset_dict_90.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e5643ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1093\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 120\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30da0877",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForSequenceClassification: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26b0d4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea2089ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):    \n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='weighted')    \n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba3b7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./DeBERTa_4_90_new\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=15,\n",
    "    save_total_limit = 5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=IntervalStrategy.STEPS,\n",
    "    eval_steps = 50,\n",
    "    #save_strategy=\"epoch\",\n",
    "    metric_for_best_model = 'f1',\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_90['train'],\n",
    "    eval_dataset=tokenized_90['valid'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71cb424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/milos.kosprdic.ivi/.local/lib/python3.7/site-packages/transformers/optimization.py:415: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1755' max='4110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1755/4110 16:58 < 22:47, 1.72 it/s, Epoch 6.40/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.066213</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.134444</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.196748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.988397</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.513614</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.369491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.057728</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.631869</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.557665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.777534</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.561190</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.624557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.923023</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.559040</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.543764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.770434</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.490835</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.541522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.640513</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.595095</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.661850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.727374</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.641500</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.638998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.706437</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.720506</td>\n",
       "      <td>0.691667</td>\n",
       "      <td>0.649547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.635349</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.813652</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.792877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.437550</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.850177</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.845291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.573756</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872071</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.581369</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.855104</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.845785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.487088</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.874426</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.873772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.838317</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.844026</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.823905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.651078</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.834600</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.830982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.530951</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.909944</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.908396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.547803</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.905074</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.663700</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.891395</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.884385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.676248</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.885481</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.882859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.659839</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.877727</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.874697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.495440</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.907147</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.901580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.475526</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.927944</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.924598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.512895</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.915197</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.908865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.875345</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.858968</td>\n",
       "      <td>0.841667</td>\n",
       "      <td>0.839662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.531519</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.920797</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.917356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.549232</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.903784</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.577855</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.915905</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.909994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.369500</td>\n",
       "      <td>0.648022</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.907342</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.638134</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.895982</td>\n",
       "      <td>0.891667</td>\n",
       "      <td>0.892164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.549088</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.925488</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.924909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.563067</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.909453</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>0.908545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.688196</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.907895</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.901720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.699736</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.904767</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.726308</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.896281</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.886163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/milos.kosprdic.ivi/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/milos.kosprdic.ivi/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/milos.kosprdic.ivi/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/milos.kosprdic.ivi/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/milos.kosprdic.ivi/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/milos.kosprdic.ivi/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "trainer.train()\n",
    "total_time = time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a9aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"DeBERTa_early4_90_new\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
